{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import timm\n",
    "import time\n",
    "import pickle\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    handlers=[logging.FileHandler('notebook.log'), logging.StreamHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seed(SEED):\n",
    "        os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_path(root_path):\n",
    "        train_path = f'{root_path}/train/'\n",
    "        test_path = f'{root_path}/test/'\n",
    "\n",
    "        return root_path, train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 13:14:50,206 - INFO - 1. Set Seed\n"
     ]
    }
   ],
   "source": [
    "CONFIG.set_seed(0xC0FFE)\n",
    "root_path, train_path, test_path = CONFIG.set_path('/root/Project/new_data')\n",
    "\n",
    "logging.info('1. Set Seed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리된 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002f99746285dfdd.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>008ccd231e1fea5d.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f5911bfda7695.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  002f99746285dfdd.jpg      16\n",
       "1  008ccd231e1fea5d.jpg      10\n",
       "2  008f5911bfda7695.jpg      10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = pd.read_csv(f'{root_path}/combined_data.csv')\n",
    "train_file.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26516</th>\n",
       "      <td>df1caa3d9f344b5a_Rotated_90.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35151</th>\n",
       "      <td>voronoi1_293316f2c9939133_Rotated__39.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35150</th>\n",
       "      <td>voronoi1_293316f2c9939133_Rotated__113.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ID  target\n",
       "26516             df1caa3d9f344b5a_Rotated_90.jpg       0\n",
       "35151   voronoi1_293316f2c9939133_Rotated__39.jpg       0\n",
       "35150  voronoi1_293316f2c9939133_Rotated__113.jpg       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_file = train_file.sort_values(by='target')\n",
    "sorted_train_file.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       0\n",
       "1  00091bffdffd83de.jpg       0\n",
       "2  00396fbc1f6cc21d.jpg       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = pd.read_csv(f'{root_path}/sample_submission.csv')\n",
    "test_file.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 데이터를 fold별로 시각화하기 위한 함수를 구성합니다.\n",
    "# Scikit-learn에서 https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html 사용한 코드를 가져와서 사용하겠습니다.\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "def plot_cv_indices(x, y, cv, ax, split_strategy='KFold', group=None, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=x, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        # print(f\"Fold {ii} :\")\n",
    "        # print(f\"  Train : index={tr[:5]}...\")\n",
    "        # print(f\"  Valid : index={tt[:5]}...\")\n",
    "        indices = np.array([np.nan] * len(x))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=0.2,\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(5))\n",
    "\n",
    "    ax.set(\n",
    "        yticks=np.arange(len(yticklabels)) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[len(yticklabels) + 0.2, -0.2],\n",
    "        xlim=[0, len(x)],\n",
    "    )\n",
    "    ax.set_title(split_strategy, fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Stratified K-Fold'}, xlabel='Sample index', ylabel='CV iteration'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_cv_indices(x=sorted_train_file['ID'],\n",
    "                y=sorted_train_file['target'],\n",
    "                cv=skf,\n",
    "                ax=ax,\n",
    "                split_strategy='Stratified K-Fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = skf.split(sorted_train_file['ID'], sorted_train_file['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 13:14:58,369 - INFO - 2. Define Dataset Class\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df.iloc[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "    \n",
    "logging.info('2. Define Dataset Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def _train_one_epoch(loader, model, optimizer, loss_fn, scheduler, scaler, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast():\n",
    "            preds = model(images)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def _val_one_epoch(loader, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 13:50:00,049 - INFO - 3. Set Hyperparameter\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 및 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'swin_large_patch4_window7_224'  # 모델명\n",
    "img_size = 224\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "patience = 3\n",
    "T_0 = 5\n",
    "T_mult = 2\n",
    "eta_min = 1e-6\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size, always_apply=True), \n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=(255, 255, 255)), \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "logging.info('3. Set Hyperparameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 13:15:02,306 - INFO - 4. Define Focal Loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92764706 2.01662404 0.95143288 0.93701723 0.93701723 0.92764706\n",
      " 0.92764706 0.91846243 0.80664962 0.92764706 0.92764706 0.95633717\n",
      " 0.92764706 1.25357711 1.8189158  0.92764706 0.95143288]\n"
     ]
    }
   ],
   "source": [
    "# 클래스 가중치 계산\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(sorted_train_file['target']), y=sorted_train_file['target'])\n",
    "print(class_weights)\n",
    "\n",
    "alpha = torch.tensor(class_weights, dtype=torch.float64).to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        # alpha 값을 targets의 크기에 맞게 브로드캐스팅\n",
    "        alpha = self.alpha.gather(0, targets.long())\n",
    "\n",
    "        focal_loss = alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            focal_loss = focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            focal_loss = focal_loss.sum()\n",
    "        return focal_loss\n",
    "    \n",
    "# loss_fn = FocalLoss(alpha=alpha, gamma=2)\n",
    "logging.info('4. Define Focal Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def tta(model, loader, device):\n",
    "    model.eval()\n",
    "    final_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            flipped_images = torch.flip(images, dims=[3])\n",
    "            outputs_flipped = model(flipped_images)\n",
    "            probabilities_flipped = torch.nn.functional.softmax(outputs_flipped, dim=1)\n",
    "\n",
    "\n",
    "            rotations = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "            rotated_probabilities = []\n",
    "\n",
    "            for angle in rotations:\n",
    "                # 정규화 된 이미지[-1, 1]에서 흰색은 (1, 1, 1)\n",
    "                rotated_images = TF.rotate(images, angle, fill=(1, 1, 1))\n",
    "                outputs_rotated = model(rotated_images)\n",
    "\n",
    "                probabilities_rotated = torch.nn.functional.softmax(outputs_rotated, dim=1)\n",
    "                rotated_probabilities.append(probabilities_rotated)\n",
    "\n",
    "            averaged_probabilities = (probabilities + probabilities_flipped + sum(rotated_probabilities)) / (len(rotations) + 2)\n",
    "            # preds = averaged_probabilities.argmax(dim=1)\n",
    "            final_preds.extend(averaged_probabilities.cpu().numpy())\n",
    "            \n",
    "    return final_preds\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(model, loader, device):\n",
    "    model.eval()\n",
    "    final_preds = []\n",
    "    rot_transform = A.Compose([\n",
    "        A.Rotate(limit=(0, 0), p=1.0, border_mode=cv2.BORDER_CONSTANT, value=(1, 1, 1))\n",
    "    ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            flipped_images = torch.flip(images, dims=[3])\n",
    "            outputs_flipped = model(flipped_images)\n",
    "            probabilities_flipped = torch.nn.functional.softmax(outputs_flipped, dim=1)\n",
    "\n",
    "            rotations = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "            rotated_probabilities = []\n",
    "            for angle in rotations:\n",
    "                rot_transform.transforms[0].limit = (angle, angle)\n",
    "                rotated_images = []\n",
    "                for img in images:\n",
    "                    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "                    rotated_img = rot_transform(image=img_np)['image']\n",
    "                    rotated_img_tensor = torch.from_numpy(rotated_img).permute(2, 0, 1).to(device)\n",
    "                    rotated_images.append(rotated_img_tensor)\n",
    "                rotated_images = torch.stack(rotated_images)\n",
    "\n",
    "                outputs_rotated = model(rotated_images)\n",
    "                probabilities_rotated = torch.nn.functional.softmax(outputs_rotated, dim=1)\n",
    "                rotated_probabilities.append(probabilities_rotated)\n",
    "\n",
    "            averaged_probabilities = (probabilities + probabilities_flipped + sum(rotated_probabilities)) / (len(rotations) + 2)\n",
    "            # preds = averaged_probabilities.argmax(dim=1)\n",
    "            final_preds.extend(averaged_probabilities.cpu().numpy())\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.base import BaseEstimator, ClassifierMixin\\n\\nclass VotingModel(BaseEstimator, ClassifierMixin):\\n    def __init__(self, estimators):\\n        super().__init__()\\n        self.estimators = estimators\\n        \\n    def fit(self, X, y=None):\\n        return self\\n    \\n    def predict(self, X):\\n        y_preds = [estimator(X).argmax(dim=1).detach().cpu().tolist() for estimator in self.estimators]\\n        return stats.mode(y_preds)[0]\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class VotingModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator(X).argmax(dim=1).detach().cpu().tolist() for estimator in self.estimators]\n",
    "        return stats.mode(y_preds)[0]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(patience, num_epochs, device):\n",
    "\n",
    "    # 시작 시간\n",
    "    since = time.time()\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    for fold_index, (train_index, validation_index) in enumerate(train_folds):\n",
    "\n",
    "        # 그라디언트 스케일러 초기화\n",
    "        scaler = GradScaler()\n",
    "  \n",
    "        print()\n",
    "        print(f'Stratified K-Fold: {fold_index}')\n",
    "        logging.info(f'Stratified K-Fold: {fold_index + 1} / 5')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model = timm.create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=17\n",
    "        ).to(device)\n",
    "\n",
    "        loss_fn = FocalLoss(alpha=alpha, gamma=2)\n",
    "        optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min)\n",
    "        \n",
    "        # train\n",
    "        train_data = sorted_train_file.iloc[train_index, :]\n",
    "        train_dataset = ImageDataset(\n",
    "            df=train_data,\n",
    "            path=train_path,\n",
    "            transform=train_transform\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        # validation\n",
    "        validation_data = sorted_train_file.iloc[validation_index, :]\n",
    "        validation_dataset = ImageDataset(\n",
    "            df=validation_data,\n",
    "            path=train_path,\n",
    "            transform=train_transform\n",
    "        )\n",
    "        validation_loader = DataLoader(\n",
    "            validation_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        best_epoch = 0\n",
    "        best_f1_score = 0\n",
    "        early_stop_counter = 0\n",
    "        best_model_weights = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            if early_stop_counter >= patience:\n",
    "                logging.info(f\"Early Stopping... epoch {epoch + 1}\")\n",
    "                print(\"Early Stopping....\\n\")\n",
    "                break\n",
    "\n",
    "            # train\n",
    "            ret = _train_one_epoch(train_loader, model, optimizer, loss_fn, scheduler, scaler, device)\n",
    "            # validation\n",
    "            ret2, val_f1 = _val_one_epoch(validation_loader, model, device)\n",
    "\n",
    "            print(f\"Loss: {ret['train_loss']:.4f}, train Accuracy: {ret['train_acc']:.4f}, train F1-Score: {ret['train_f1']:.4f}\")\n",
    "            print(f\"validation Accuracy: {ret2['train_acc']:.4f}, validation F1-Score: {ret2['train_f1']:.4f}\")\n",
    "            print('-' * 10)\n",
    "            \n",
    "            # f1-score을 비교\n",
    "            if val_f1 > best_f1_score:\n",
    "                early_stop_counter = 0\n",
    "\n",
    "                best_epoch = epoch\n",
    "                best_f1_score = val_f1\n",
    "                best_model_weights = model.state_dict()\n",
    "                \n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "    \n",
    "        print(f'best epoch: {best_epoch}, best f1 score: {best_f1_score}')\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "        # 가장 좋은 모델의 가중치(w) 가져오기\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "        model_path = f'{root_path}/model/{model_name}_all'\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "        with open(f'{model_path}/fold_{fold_index}' + '.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        models.append({\n",
    "            'model': model,\n",
    "            'weights': best_model_weights,\n",
    "            'f1_score': best_f1_score,\n",
    "        })\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('6. Start Training')\n",
    "logging.info('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련하기\n",
    "models = train_model(patience, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.info('----------------------------------------------')\n",
    "logging.info('7. Finish Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    with open(f'{root_path}/model/{model_name}_all/fold_{i}.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    models.append(model) \n",
    "\n",
    "# model = VotingModel(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "어떤 클래스를 잘 맞추고, 잘 못맞추는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_cm(model:VotingModel, sample_dataloader, device):\n",
    "    total_labels = []\n",
    "    total_preds = []\n",
    "\n",
    "    for images, labels in tqdm(sample_dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        predicts = model.predict(images)\n",
    "\n",
    "        total_preds.extend(predicts.tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "    total_preds = np.array(total_preds)\n",
    "    total_labels = np.array(total_labels)\n",
    "    _f1_score = f1_score(total_labels, total_preds, average='macro')\n",
    "    print(\"Model f1_score : \", _f1_score)\n",
    "\n",
    "    cm = confusion_matrix(total_labels, total_preds)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 클래스 별 20% 랜덤 추출\n",
    "sample_file = pd.DataFrame(columns=['ID', 'target'])\n",
    "\n",
    "for i in range(17):\n",
    "    temp = sorted_train_file.loc[sorted_train_file['target'] == i].sample(frac=0.2, random_state=0xC0FFE)\n",
    "    sample_file = pd.concat([sample_file, temp], axis=0)\n",
    "    \n",
    "\n",
    "sample_dataset = ImageDataset(\n",
    "    df=sample_file,\n",
    "    path=train_path,\n",
    "    transform=train_transform\n",
    ")\n",
    "sample_loader = DataLoader(\n",
    "    sample_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "# plot_cm(model, sample_loader, device)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference & Submisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(\n",
    "    df=test_file,\n",
    "    path=test_path,\n",
    "    transform=test_transform,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = f\"{root_path}/result\"\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "# Soft Voting\n",
    "ensemble_preds_soft = []\n",
    "for model_info in models:\n",
    "    # shape (model 수, test data 이미지 수, class 수) --> (5, 3140, 17)\n",
    "    ensemble_preds_soft.append(tta(model_info['model'], test_loader, device))\n",
    "\n",
    "# shape (test data 이미지 수, class 수) --> (3140, 17)\n",
    "ensemble_preds_soft = np.mean(ensemble_preds_soft, axis=0)\n",
    "# shape (test data 이미지 수) --> (3140, )\n",
    "final_preds_soft = np.argmax(ensemble_preds_soft, axis=1)\n",
    "\n",
    "submission_df_soft = pd.read_csv(f'{root_path}/sample_submission.csv')\n",
    "submission_df_soft['target'] = final_preds_soft\n",
    "submission_df_soft.to_csv(f'{result_path}/{model_name}_submission_soft.csv', index=False)\n",
    "\n",
    "\n",
    "# Weighted Voting\n",
    "weights = [model_info['f1_score'] for model_info in models]\n",
    "weights = np.array(weights) / sum(weights)\n",
    "\n",
    "ensemble_preds_weighted = []\n",
    "for model_info, weight in zip(models, weights):\n",
    "    ensemble_preds_weighted.append(weight * np.array(tta(model_info['model'], test_loader, device)))\n",
    "\n",
    "ensemble_preds_weighted = np.sum(ensemble_preds_weighted, axis=0)\n",
    "final_preds_weighted = np.argmax(ensemble_preds_weighted, axis=1)\n",
    "\n",
    "submission_df_weighted = pd.read_csv(f'{root_path}/sample_submission.csv')\n",
    "submission_df_weighted['target'] = final_preds_weighted\n",
    "submission_df_weighted.to_csv(f'{result_path}/{model_name}_submission_weighted.csv', index=False)\n",
    "\n",
    "logging.info('8. Predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresult_path = f\"{root_path}/result\"\\nif not os.path.exists(result_path):\\n    os.makedirs(result_path)\\n\\n# Soft Voting\\nensemble_preds_soft = []\\nfor model in models:\\n    # shape (model 수, test data 이미지 수, class 수) --> (5, 3140, 17)\\n    ensemble_preds_soft.append(tta2(model, test_loader, device))\\n    \\n# shape (test data 이미지 수, class 수) --> (3140, 17)\\nensemble_preds_soft = np.mean(ensemble_preds_soft, axis=0)\\n# shape (test data 이미지 수) --> (3140, )\\nfinal_preds_soft = np.argmax(ensemble_preds_soft, axis=1)\\n\\nsubmission_df_soft = pd.read_csv(f\\'{root_path}/sample_submission.csv\\')\\nsubmission_df_soft[\\'target\\'] = final_preds_soft\\nsubmission_df_soft.to_csv(f\\'{result_path}/{model_name}_submission_soft.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 가져왔을 때\n",
    "\n",
    "'''\n",
    "result_path = f\"{root_path}/result\"\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "# Soft Voting\n",
    "ensemble_preds_soft = []\n",
    "for model in models:\n",
    "    # shape (model 수, test data 이미지 수, class 수) --> (5, 3140, 17)\n",
    "    ensemble_preds_soft.append(tta2(model, test_loader, device))\n",
    "    \n",
    "# shape (test data 이미지 수, class 수) --> (3140, 17)\n",
    "ensemble_preds_soft = np.mean(ensemble_preds_soft, axis=0)\n",
    "# shape (test data 이미지 수) --> (3140, )\n",
    "final_preds_soft = np.argmax(ensemble_preds_soft, axis=1)\n",
    "\n",
    "submission_df_soft = pd.read_csv(f'{root_path}/sample_submission.csv')\n",
    "submission_df_soft['target'] = final_preds_soft\n",
    "submission_df_soft.to_csv(f'{result_path}/{model_name}_submission_soft.csv', index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
